{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('protopnext_userstudygui/protopnext')\n",
    "sys.path.append('protopnext_userstudygui/protopnext/protopnet')\n",
    "sys.path.append('explain_dataset/protopnext/protopnet/utilities')\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from general_utilities import find_high_activation_crop\n",
    "from project_utilities import custom_unravel_index, hash_func\n",
    "from visualization_utilities import indices_to_upsampled_boxes\n",
    "import numpy as np\n",
    "from protopnet.visualization import *\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from torchvision import transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f74ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_done(model_path):\n",
    "    model_id = '_'.join(model_path.split('/')[-2:])[:-4]\n",
    "    save_folder = Path(f'protopnext_userstudygui/image_folderv5/{model_id}')    \n",
    "    return os.path.exists(str(save_folder / 'all_boxes.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protopnet.datasets import cub200, cub200_cropped\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "split_dataloader = cub200_cropped.train_dataloaders(\n",
    "    batch_sizes={\"train\": batch_size, \"project\": batch_size, \"val\": batch_size},\n",
    "    data_path = os.environ.get(\"CUB200_DIR\"), \n",
    ")\n",
    "\n",
    "from protopnet.preprocess import mean, std\n",
    "from protopnet.datasets.torch_extensions import ImageFolderDict\n",
    "\n",
    "push_dataset_path = pathlib.Path(\n",
    "    os.environ.get(\"CUB200_DIR\") + \"/train_cropped\"\n",
    ")\n",
    "img_size = 224\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "push_dataset = ImageFolderDict(\n",
    "    push_dataset_path,\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "loader_config = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 29,\n",
    "    \"pin_memory\": False,\n",
    "    \"prefetch_factor\": 8,\n",
    "}\n",
    "\n",
    "s_dataset = push_dataset  # Subset(push_dataset, range(10))\n",
    "push_loader = torch.utils.data.DataLoader(s_dataset, **loader_config)\n",
    "\n",
    "\n",
    "val_dataset_path = pathlib.Path(\n",
    "    os.environ.get(\"CUB200_DIR\") + \"/val_cropped\"\n",
    ")\n",
    "img_size = 224\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "val_dataset = ImageFolderDict(\n",
    "    val_dataset_path,\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "loader_config = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 29,\n",
    "    \"pin_memory\": False,\n",
    "    \"prefetch_factor\": 8,\n",
    "}\n",
    "val_no_aug_loader = torch.utils.data.DataLoader(val_dataset, **loader_config)\n",
    "\n",
    "\n",
    "val_loader = split_dataloader.val_loader\n",
    "push_loader = split_dataloader.project_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dee3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cropped_models_infos = pd.read_csv(\"model-pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(cropped_models_infos)):\n",
    "    print(cropped_models_infos['best[prototypes_embedded]/eval/accuracy_cos'][idx], \\\n",
    "          cropped_models_infos['best[prototypes_embedded]/eval/accuracy_l2'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cropped_models_infos.copy()\n",
    "\n",
    "df = df.sort_values(by=[\"backbone\", \"best[prototypes_embedded]/eval/accuracy_cos\"], ascending=[True, False])\n",
    "df['group'] = df.groupby('backbone').cumcount()\n",
    "cropped_models_infos = df.sort_values(by=\"group\").drop(columns=[\"group\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1372cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_model_paths = []\n",
    "for idx in range(len(cropped_models_infos)):\n",
    "    print(cropped_models_infos['backbone'][idx], cropped_models_infos['best[prototypes_embedded]/eval/accuracy_cos'][idx], \\\n",
    "          cropped_models_infos['best[prototypes_embedded]/eval/accuracy_l2'][idx])\n",
    "    all_model_paths.append([cropped_models_infos['best_model_cos'].values[idx], idx, cropped_models_infos['backbone'].values[idx], 'cos'])\n",
    "    all_model_paths.append([cropped_models_infos['best_model_l2'].values[idx], idx, cropped_models_infos['backbone'].values[idx], 'l2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fdd3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(all_model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ae79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_indices_by_percentile(array, num_samples):\n",
    "    np.random.seed(42)\n",
    "    ret = dict()\n",
    "    for i in range(10):\n",
    "        low = np.percentile(array, i * 10)\n",
    "        high = np.percentile(array, (i + 1) * 10)\n",
    "        indices = np.where((array >= low) & (array < high))[0]\n",
    "        if len(indices) > 0:\n",
    "            sampled_indices = np.random.choice(indices, min(num_samples, len(indices)), replace=False)\n",
    "            ret[i] = sampled_indices\n",
    "        else:\n",
    "            ret[i] = np.array([])\n",
    "\n",
    "    return ret\n",
    "\n",
    "def create_lookup_table(index_dict):\n",
    "    lookup = {}\n",
    "    for key, indices in index_dict.items():\n",
    "        for idx in indices:\n",
    "            lookup[idx] = key\n",
    "    return lookup\n",
    "\n",
    "def percentile(tensor: torch.Tensor, value: float):\n",
    "    sorted_tensor = torch.sort(tensor).values  # Sort the tensor\n",
    "    rank = torch.searchsorted(sorted_tensor, torch.tensor(value), right=True)  # Find the position\n",
    "    percentile = (rank / len(tensor)) * 100  # Convert rank to percentile\n",
    "    return percentile.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5dddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_bounding_box(heatmap, percentile=95):\n",
    "    \"\"\"\n",
    "    Finds the bounding box (min_x, min_y, max_x, max_y) of pixels with values >= the given percentile.\n",
    "\n",
    "    Parameters:\n",
    "    - heatmap (np.ndarray): 2D numpy array representing the heatmap.\n",
    "    - percentile (float): Percentile threshold (default: 95).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (min_x, min_y, max_x, max_y) coordinates of the bounding box.\n",
    "             Returns None if no pixels exceed the threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the threshold value for the given percentile\n",
    "    threshold = np.percentile(heatmap, percentile)\n",
    "\n",
    "    # Get coordinates of pixels that meet the threshold\n",
    "    y_indices, x_indices = np.where(heatmap >= threshold)\n",
    "    \n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        print('Issue found here', flush=True)\n",
    "        return [-1, -1, -1, -1]  # No pixels exceed the threshold\n",
    "\n",
    "    # Bounding box coordinates\n",
    "    min_x, max_x = np.min(x_indices), np.max(x_indices)\n",
    "    min_y, max_y = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "    return min_x, min_y, max_x, max_y\n",
    "\n",
    "def get_all_img_boxes(\n",
    "    model,\n",
    "    img_idx,\n",
    "    ori_path,\n",
    "    target_proto_idx,\n",
    "    eval_dataloader,\n",
    "    act_percentile,\n",
    "    act_val,\n",
    "    eval_dataloader_no_aug,\n",
    "    img_size,\n",
    "    device,\n",
    "    all_latent_space_size_activation_maps,\n",
    "    proto_rank=-1\n",
    "):\n",
    "\n",
    "    latent_space_size_activation_maps = all_latent_space_size_activation_maps[img_idx]\n",
    "    proto_actmaps_on_image = latent_space_size_activation_maps[\n",
    "        target_proto_idx, :, :\n",
    "    ].unsqueeze(0).unsqueeze(0)\n",
    "    proto_actmaps_on_image_normed = (\n",
    "        proto_actmaps_on_image - proto_actmaps_on_image.min()\n",
    "    ) / (proto_actmaps_on_image.max() - proto_actmaps_on_image.min())\n",
    "    img_height, img_width = img_size, img_size\n",
    "\n",
    "    proto_actmaps_on_image_normed = \\\n",
    "        torch.nn.Upsample(\n",
    "            size=(img_height, img_width), mode=\"bilinear\", align_corners=False\n",
    "        )(proto_actmaps_on_image_normed).squeeze(1).clone().detach().cpu().numpy()\n",
    "    \n",
    "    (\n",
    "        proto_part_left_x,\n",
    "        proto_part_upper_y,\n",
    "        proto_part_right_x,\n",
    "        proto_part_lower_y,\n",
    "    ) = get_bounding_box(proto_actmaps_on_image_normed.squeeze(0), percentile=95)\n",
    "\n",
    "    return [img_idx, ori_path, target_proto_idx, proto_rank, act_percentile, act_val, proto_part_left_x,\n",
    "        proto_part_upper_y,\n",
    "        proto_part_right_x,\n",
    "        proto_part_lower_y,]\n",
    "\n",
    "def process_model(model_path):\n",
    "    \n",
    "    model_id = '_'.join(model_path.split('/')[-2:])[:-4]\n",
    "    save_folder = Path(f'protopnext_userstudygui/image_folderv5/{model_id}')\n",
    "    print(f\"Processing {model_id}, at {save_folder}\", flush=True)\n",
    "    os.makedirs(str(save_folder), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(str(save_folder / 'all_boxes.npy')):\n",
    "        print(\"Skipping\")\n",
    "        print(os.system(f\"stat {str(save_folder / 'all_boxes.npy')}\"))\n",
    "        return\n",
    "    \n",
    "    model = torch.load(model_path)\n",
    "    model = model.to('cuda')\n",
    "    \n",
    "    model = reproject_prototypes(model, save_folder, push_loader, device='cuda')\n",
    "    model.prune_prototypes()\n",
    "    \n",
    "    if not os.path.exists(str(save_folder / 'prototypes/patch_info_dict.json')):\n",
    "    \n",
    "        save_prototype_images_to_file(\n",
    "            model=model,\n",
    "            std=split_dataloader.std,\n",
    "            mean=split_dataloader.mean,\n",
    "            push_dataloader=push_loader,\n",
    "            save_loc=save_folder,\n",
    "            img_size=(img_size, img_size),\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "    \n",
    "    device = 'cuda'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        similarity_score_to_each_prototypes = []\n",
    "        all_latent_space_size_activation_maps = []\n",
    "        all_paths = []\n",
    "        for batch_data_dict in tqdm(val_loader, total=len(val_loader), leave=False, desc=\"Extract feats\"):\n",
    "            batch_images = batch_data_dict[\"img\"].to(device)\n",
    "            batch_paths = batch_data_dict[\"path\"]\n",
    "            model_outputs = model(\n",
    "                batch_images,\n",
    "                return_prototype_layer_output_dict=True,\n",
    "                return_similarity_score_to_each_prototype=True,\n",
    "            )\n",
    "            # batch size, num_protos, latent_height, latent_width\n",
    "            latent_space_size_activation_maps = model_outputs[\"prototype_activations\"]\n",
    "\n",
    "            # batch size, num_protos\n",
    "            similarity_score_to_each_prototype = model_outputs[\n",
    "                \"similarity_score_to_each_prototype\"\n",
    "            ]\n",
    "\n",
    "            for p in batch_paths:\n",
    "                all_paths.append(p)\n",
    "\n",
    "            all_latent_space_size_activation_maps.append(latent_space_size_activation_maps.detach().cpu())\n",
    "            similarity_score_to_each_prototypes.append(similarity_score_to_each_prototype.detach().cpu())\n",
    "            \n",
    "            del batch_images, model_outputs\n",
    "\n",
    "    similarity_score_to_each_prototypes = torch.cat(similarity_score_to_each_prototypes, dim=0)\n",
    "    all_latent_space_size_activation_maps = torch.cat(all_latent_space_size_activation_maps, dim=0)\n",
    "    print(similarity_score_to_each_prototypes.shape, all_latent_space_size_activation_maps.shape)\n",
    "    \n",
    "    proto_sorted_idxs = torch.argsort(similarity_score_to_each_prototypes, dim=1, descending=True)\n",
    "    all_boxes = []\n",
    "    for img_idx, proto_idxs in tqdm(enumerate(proto_sorted_idxs), total=len(proto_sorted_idxs)):\n",
    "\n",
    "        found = False\n",
    "        for qc, proto_idx in enumerate(proto_idxs):\n",
    "            if found:\n",
    "                break\n",
    "            act_val = similarity_score_to_each_prototypes[img_idx][proto_idx]\n",
    "            act_perc = percentile(similarity_score_to_each_prototypes[:, proto_idx], act_val)\n",
    "\n",
    "            if act_perc < 90:\n",
    "                found = True\n",
    "\n",
    "\n",
    "            all_boxes.append(get_all_img_boxes(\n",
    "                model,\n",
    "                img_idx=img_idx,\n",
    "                ori_path=all_paths[img_idx],\n",
    "                proto_rank=qc,\n",
    "                target_proto_idx=proto_idx,\n",
    "                eval_dataloader=val_loader,\n",
    "                act_percentile=act_perc,\n",
    "                act_val=act_val.item(),\n",
    "                eval_dataloader_no_aug=val_no_aug_loader,\n",
    "                img_size=224,\n",
    "                device='cuda',\n",
    "                all_latent_space_size_activation_maps=all_latent_space_size_activation_maps\n",
    "            ))\n",
    "    \n",
    "    np.save(save_folder / 'all_boxes.npy', all_boxes)\n",
    "    \n",
    "    del model, proto_sorted_idxs, all_boxes, similarity_score_to_each_prototypes, all_latent_space_size_activation_maps\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "for model_path, row_idx, backbone, dist_metric in tqdm(all_model_paths):\n",
    "    print(model_path, row_idx, backbone, dist_metric)\n",
    "    process_model(model_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    finished_rows_info = []\n",
    "    finished_l2_only = []\n",
    "    finished_cos_only = []\n",
    "\n",
    "    for row_idx in range(len(cropped_models_infos)):\n",
    "\n",
    "        backbone = cropped_models_infos['backbone'].values[row_idx]\n",
    "        model_path_cos = cropped_models_infos['best_model_cos'].values[row_idx]\n",
    "        model_path_l2 = cropped_models_infos['best_model_l2'].values[row_idx]\n",
    "\n",
    "        done_cos = check_done(model_path_cos)\n",
    "        done_l2 = check_done(model_path_l2)\n",
    "\n",
    "        if done_cos and done_l2:\n",
    "            finished_rows_info.append([row_idx, backbone, model_path_cos, model_path_l2])\n",
    "\n",
    "        if done_cos and not done_l2: finished_cos_only.append([row_idx, backbone, model_path_cos, model_path_l2])\n",
    "        if done_l2 and not done_cos: finished_l2_only.append([row_idx, backbone, model_path_cos, model_path_l2])\n",
    "    \n",
    "    print(len(finished_rows_info), len(finished_cos_only), len(finished_l2_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_counts = dict()\n",
    "for row_idx, backbone, model_path_cos, model_path_l2 in finished_rows_info:\n",
    "    \n",
    "    if backbone in backbone_counts:\n",
    "        backbone_counts[backbone].add(row_idx)\n",
    "    else:\n",
    "        backbone_counts[backbone] = set([row_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaacda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(backbone_counts['densenet161']), len(backbone_counts['vgg19']), len(backbone_counts['resnet50[pretraining=inaturalist]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ae525",
   "metadata": {},
   "source": [
    "# process boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_vectorized(boxes1, boxes2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Vectorized IoU computation for two sets of boxes in format: [x1, y1, x2, y2].\n",
    "    boxes1: shape (N, 4), boxes2: shape (M, 4)\n",
    "    Returns: IoU matrix of shape (N, M).\n",
    "    \"\"\"\n",
    "    x1 = np.maximum(boxes1[:, 0][:, None], boxes2[:, 0][None, :])\n",
    "    y1 = np.maximum(boxes1[:, 1][:, None], boxes2[:, 1][None, :])\n",
    "    x2 = np.minimum(boxes1[:, 2][:, None], boxes2[:, 2][None, :])\n",
    "    y2 = np.minimum(boxes1[:, 3][:, None], boxes2[:, 3][None, :])\n",
    "\n",
    "    inter_width = np.maximum(0, x2 - x1)\n",
    "    inter_height = np.maximum(0, y2 - y1)\n",
    "    intersection = inter_width * inter_height\n",
    "\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    union = area1[:, None] + area2[None, :] - intersection\n",
    "    iou = intersection / (union + eps)\n",
    "    return iou\n",
    "\n",
    "def scale_bounding_box(avg_left_x, avg_upper_y, avg_right_x, avg_lower_y, H, W):\n",
    "    \n",
    "    scale_x = W / 224.0\n",
    "    scale_y = H / 224.0\n",
    "\n",
    "    left_x = avg_left_x * scale_x\n",
    "    upper_y = avg_upper_y * scale_y\n",
    "    right_x = avg_right_x * scale_x\n",
    "    lower_y = avg_lower_y * scale_y\n",
    "\n",
    "    return int(left_x), int(upper_y), int(right_x), int(lower_y)\n",
    "\n",
    "\n",
    "def plot_scaled_bounding_boxes(image, avg_left_x, avg_upper_y, avg_right_x, avg_lower_y):\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    up_left_x, up_upper_y, up_right_x, up_lower_y = scale_bounding_box(\n",
    "        avg_left_x, avg_upper_y, avg_right_x, avg_lower_y, H, W\n",
    "    )\n",
    "\n",
    "    image_upscaled = image.copy()\n",
    "    cv2.rectangle(image_upscaled, (up_left_x, up_upper_y), (up_right_x, up_lower_y), (255, 0, 0), 2)\n",
    "\n",
    "    image_downscaled = cv2.resize(image_upscaled, (224, 224))\n",
    "\n",
    "    down_left_x = int(avg_left_x)\n",
    "    down_upper_y = int(avg_upper_y)\n",
    "    down_right_x = int(avg_right_x)\n",
    "    down_lower_y = int(avg_lower_y)\n",
    "    cv2.rectangle(image_downscaled, (down_left_x, down_upper_y), (down_right_x, down_lower_y), (0, 255, 0), 2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(cv2.cvtColor(image_upscaled, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f'Upscaled Image ({H}x{W})')\n",
    "\n",
    "    axes[1].imshow(cv2.cvtColor(image_downscaled, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title('Downscaled Image (224x224)')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800234b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Start')\n",
    "\n",
    "def process_packet(all_boxes, patch_loc):\n",
    "    box_info_dict = dict()\n",
    "    for x in tqdm(all_boxes):\n",
    "        \n",
    "        # get this image's box info\n",
    "        img_idx, ref_img_path, target_proto_idx, proto_rank, act_percentile, act_val,\\\n",
    "        proto_part_left_x,\\\n",
    "        proto_part_upper_y,\\\n",
    "        proto_part_right_x,\\\n",
    "        proto_part_lower_y = x\n",
    "        \n",
    "        # get proto info\n",
    "        proto_idx = str(target_proto_idx.split('(')[-1].split(')')[0])\n",
    "        proto_part_lower_y1, proto_part_upper_y1, proto_part_right_x1, proto_part_left_x1, proto_img_path = patch_loc[proto_idx]\n",
    "        proto_part_left_x,\\\n",
    "        proto_part_upper_y,\\\n",
    "        proto_part_right_x,\\\n",
    "        proto_part_lower_y = \\\n",
    "        int(proto_part_left_x),\\\n",
    "        int(proto_part_upper_y),\\\n",
    "        int(proto_part_right_x),\\\n",
    "        int(proto_part_lower_y)\n",
    "        \n",
    "        # assemble boxes\n",
    "        ref_box = [proto_part_left_x,\\\n",
    "                        proto_part_upper_y,\\\n",
    "                        proto_part_right_x,\\\n",
    "                        proto_part_lower_y]\n",
    "        \n",
    "        proto_box = [proto_part_left_x1, proto_part_upper_y1, proto_part_right_x1, proto_part_lower_y1]\n",
    "        \n",
    "        part_left_x, part_right_x = sorted([ref_box[0], ref_box[2]])\n",
    "        part_upper_y, part_lower_y = sorted([ref_box[1], ref_box[3]])\n",
    "\n",
    "        proto_part_left_x1, proto_part_right_x1 = sorted([proto_box[0], proto_box[2]])\n",
    "        proto_part_upper_y1, proto_part_lower_y1 = sorted([proto_box[1], proto_box[3]])\n",
    "        \n",
    "        # assemble packet\n",
    "        packet = {\n",
    "            'img_idx': img_idx,\n",
    "            'ref_path': ori_path,\n",
    "            'proto_idx': proto_idx,\n",
    "            'proto_path': proto_img_path,\n",
    "            'ref_box': [part_left_x, part_upper_y, part_right_x, part_lower_y],\n",
    "            'proto_box': [proto_part_left_x1, proto_part_upper_y1, proto_part_right_x1, proto_part_lower_y1],\n",
    "            'proto_rank': proto_rank,\n",
    "            'act_percentile': act_percentile,\n",
    "            'act_val': act_val,\n",
    "        }\n",
    "        \n",
    "        if img_idx in box_info_dict:\n",
    "            box_info_dict[img_idx].append(packet)\n",
    "        else:\n",
    "            box_info_dict[img_idx] = [packet]\n",
    "            # print(img_idx, ref_img_path)\n",
    "    \n",
    "    return box_info_dict\n",
    "\n",
    "\n",
    "all_info_dict = dict()\n",
    "for row_idx in tqdm(range(len(cropped_models_infos))):\n",
    "    \n",
    "    backbone = cropped_models_infos['backbone'].values[row_idx]\n",
    "    model_path1, model_path2 = cropped_models_infos['best_model_cos'][row_idx], cropped_models_infos['best_model_l2'][row_idx]\n",
    "    model_id1 = '_'.join(model_path1.split('/')[-2:])[:-4]\n",
    "    save_folder1 = Path(f'protopnext_userstudygui/image_folderv5/{model_id1}')\n",
    "    model_id2 = '_'.join(model_path2.split('/')[-2:])[:-4]\n",
    "    save_folder2 = Path(f'protopnext_userstudygui/image_folderv5/{model_id2}')\n",
    "    print(save_folder1, save_folder2)\n",
    "    \n",
    "    done_cos = check_done(model_path1)\n",
    "    done_l2 = check_done(model_path2)\n",
    "    \n",
    "    if done_cos and done_l2:\n",
    "        \n",
    "        all_boxes1 = np.load(save_folder1 / 'all_boxes.npy')\n",
    "        all_boxes2 = np.load(save_folder2 / 'all_boxes.npy')\n",
    "        \n",
    "        with open (f\"{save_folder1}/prototypes/patch_info_dict.json\", 'r') as f:\n",
    "            patch_loc1 = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        with open (f\"{save_folder2}/prototypes/patch_info_dict.json\", 'r') as f:\n",
    "            patch_loc2 = json.load(f)\n",
    "        f.close()\n",
    "        ref_id = f\"row{row_idx}_img_idx{img_idx}_{model_id1}_{model_id2}_{backbone}\"\n",
    "        print(ref_id)\n",
    "        cos_packet = process_packet(all_boxes1, patch_loc1)\n",
    "        \n",
    "        l2_packet = process_packet(all_boxes2, patch_loc2)\n",
    "        \n",
    "        \n",
    "        all_info_dict[ref_id] = dict()\n",
    "        all_info_dict[ref_id]['cos'] = cos_packet\n",
    "        all_info_dict[ref_id]['l2'] = l2_packet\n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_dict[list(all_info_dict.keys())[0]]['cos']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_dict[list(all_info_dict.keys())[0]]['cos']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3717600",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_v5.json', 'w') as f:\n",
    "    json.dump(all_info_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
